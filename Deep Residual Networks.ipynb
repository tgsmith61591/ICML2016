{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Networks\n",
    "\n",
    "[Kaiming He](http://kaiminghe.com/index.html)\n",
    "\n",
    "06/19/2016\n",
    "\n",
    "[Tutorial page](http://kaiminghe.com/icml16tutorial/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abstract__\n",
    "\n",
    "*Deeper neural networks are more difficult to train. Beyond a certain depth, traditional deeper networks start to show severe underfitting caused by optimization difficulties. This tutorial will describe the recently developed residual learning framework, which eases the training of networks that are substantially deeper than those used previously. These residual networks are easier to converge, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with depth of up to 152 layers --- 8x deeper than VGG nets but still having lower complexity. These deep residual networks are the foundations of our 1st-place winning entries in all five main tracks in ImageNet and COCO 2015 competitions, which cover image classification, object detection, and semantic segmentation.*\n",
    "\n",
    "*In this tutorial we will further look into the propagation formulations of residual networks. Our latest work reveals that when the residual networks have identity mappings as skip connections and inter-block activations, the forward and backward signals can be directly propagated from one block to any other block. This leads us to promising results of 1001-layer residual networks. Our work suggests that there is much room to exploit the dimension of network depth, a key to the success of modern deep learning.*\n",
    "\n",
    "__Papers__\n",
    "\n",
    "*[Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)*\n",
    "\n",
    "*[Identity Mappings in Deep Residual Networks](http://arxiv.org/abs/1603.05027)*\n",
    "\n",
    "*Watch a condensed version of this lecture [here](https://www.youtube.com/watch?v=1PGLj-uKT1w&feature=youtu.be).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ResNet* (100 - 1000 layers)\n",
    "\n",
    "- Deepest layers (ImageNet) in 2014: 20 layers\n",
    "- Winning network: 152 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training deep residual networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of images, and a set of 3 - 5 classification labels:\n",
    "\n",
    "- Naïve approach:\n",
    "  - SVM\n",
    "  - Pixels as input\n",
    "\n",
    "- More sophisticated, traditional image recognition:\n",
    "  - Edge detection\n",
    "  - Color normalization\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the pipeline as a 4 - 5 layer process. In a traditional image classification pipeline, we generally require some domain expertise; in a deep learning domain, we typically require less.\n",
    "\n",
    "If we want to train layers with over, say, 10 layers, 30 layers, etc., sometimes we require some skipped connections. Furthermore, if we need over 100 layers (up to 1000 layers!) we need to design our skips carefully.\n",
    "\n",
    "## Say we have a linear activation function:\n",
    "\n",
    "__Backing up to a single layer__:\n",
    "\n",
    "- Generally computed as a matrix multiplication\n",
    "- If activation is simply linear (with a single layer), variance of input layer is proportional to the variance of the output layer\n",
    "    - If there are more layers, the variance in the input layer is proportional to the variance of the output layer multiplied by the product of the hidden layers\n",
    "\n",
    "__Controlling the variance of the network:__\n",
    "\n",
    "- If the variance of each layer is slightly smaller than an ideal value, your gradient will dimish\n",
    "- If the variance of each layer is slightly larger than an ideal value, your gradient will explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialization__\n",
    "\n",
    "- Need healthy forward and backward propagation\n",
    "  - The number of input nodes multiplied by the variance of the weights should equal 1\n",
    "  - If you have a healthy forward propagation signal, you'll typically have a healthy backpropagation signal\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now say we are using ReLu activation\n",
    "\n",
    "__Initialization__:\n",
    "\n",
    "- The number of inputs in each layer multiplied the weights should *equal 2* instead of 1.\n",
    "- If you do something wrong in any layer, the result is amplified exponentially throughout the forward propagation\n",
    "- A better initialization will help improve convergence\n",
    "- The deeper the network, the more important the initialization criteria\n",
    "  - Lest you destroy the ability for the network to converge\n",
    "- [Batch normalization paper](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf) says we should normalize the input and output of each layer\n",
    "  - Critical for faster training of deep neural nets\n",
    "\n",
    "__Batch normalization__:\n",
    "\n",
    "$$X = \\frac{x_{i}-\\mu}{\\sigma}$$\n",
    "  - In training mode, σ and µ contribute to the back-propagation\n",
    "- Easy to make errors in batch normalization\n",
    "- Greatly accelerates the training of deep neural networks\n",
    "- Improves accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper\n",
    "\n",
    "*Is learning better networks as simple as stacking more layers?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply stacking does not improve our performance:\n",
    "\n",
    "![Image](img/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](img/stacking2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This motivates us to develop DRN*\n",
    "\n",
    "__Rather than fit *H(X)*, fit *F(X)*, where *H(X) = F(X) + x*__&mdash;this approaches an identity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our network has many, many layers. We can expect that each layer will do less than those in a shallow network. In this case, our layers should do more identity mapping.\n",
    "\n",
    "Before the prevalence of deep learning, the most successful hand-crafted features encoded the vectors with respect to a \"dictionary\" (residual representations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network design\n",
    "\n",
    "Each layer is:\n",
    "  - 3X3 conv\n",
    "  - spatial size / 2\n",
    "  - simple design, __just deep__\n",
    "  - skipped connections skip layers\n",
    "    - sometimes three\n",
    "    - uses a bottle neck structure:\n",
    "      - alternating layers learn compressed feature spaces\n",
    "  \n",
    "Furthermore:\n",
    "  - No max pooling (almost)\n",
    "  - No hidden fc\n",
    "  - No dropout\n",
    "  - Only one fully connected layer\n",
    "  \n",
    "__Training__\n",
    "  - Trained from *scratch*\n",
    "  - Use batch normalization\n",
    "  - Standard hyper-parameters\n",
    "  \n",
    "![Image](img/resnet1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__ResNet loss on ImageNet is nearly half that of its second-place competitor__*\n",
    "\n",
    "Caveats of deep learning:\n",
    "  - Deeper networks are harder to optimize\n",
    "    - Even if the solution exists, the solver may not find it\n",
    "  - Generalizability\n",
    "    - How good will net generalize to test/validation data?\n",
    "    - Increasing the width can improve training error, but damages test performance\n",
    "    \n",
    "How does DRN solve these?\n",
    "  - Improves the optimization\n",
    "  - Does __not__ address generalizability\n",
    "    - Although optimization improvements allow for deeper, thinner networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going from 100 layers to 1000 layers\n",
    "\n",
    "__On identity mappings for optimization__\n",
    "  - shortcut mapping: h = identity\n",
    "  - very smooth forward propagation\n",
    "      - response of any layer is equal to the response of any shallower layer + the summation of a set of residual functions\n",
    "      - the feature of any deeper layer is an additive outcome\n",
    "  - very smooth back propagation\n",
    "      - the gradient of any shallower layer is an additive outcome of the deeper layers\n",
    "      - unlikely for this gradient to diminish\n",
    "          - the presence of a multiplicative outcome directly causes the diminishing gradient problem\n",
    "          \n",
    "          \n",
    "__Experiments (all performed on ResNet with 100 layers)__\n",
    "\n",
    "Results of the shortcut connection:\n",
    "  - What if a shortcut ≠ identity?\n",
    "      - The residual function was scaled by a constant 0.5\n",
    "          - Much higher error&mdash;nearly 3-4 times greater\n",
    "              - Multiplicative effect exploded the gradient\n",
    "      - 1x1 conv layer followed by sigmoid activation\n",
    "          - Error was about 2.5 times greater\n",
    "          - Kept the bottleneck 3x3 conv structure\n",
    "      - Made the shortcut function a 1x1 conv layer\n",
    "          - Still higher error\n",
    "      - Shortcut functions are blocked by some kind of multiplication. This means that if the shortcut mapping is multiplicative, there is a chance that the direct propagation is decayed. For instance, if the shortcut is a constant scaling of the input signal, we can show that the forward propagation will explode the gradient exponentially.\n",
    "      - Bias initialized at 0\n",
    "          - If you initialize the bias to be a very negative number, the gate becomes 1, which will exhibit similar results to the identity-skip formulation\n",
    "  - What if the shortcut = identity or ReLu (special gate: either multiplies signal by 1 or 0)?\n",
    "      - 1st design: weight layer, batch norm, 3x3 conv, batch norm and elementwise addition\n",
    "      - 2nd design: same, but move the last batch norm step to after the elementwise addition\n",
    "          - weight layer, batch norm, 3x3 conv, elementwise addition, batch norm\n",
    "      - 3rd design: move elementwise addition before the weight layer\n",
    "          - elementwise addition, weight layer, batch norm, 3x3 conv, batch norm\n",
    "          \n",
    "Increasing layers from 100 to 200 degraded performance, but using the improved activation function and going even deeper, the performance improved dramatically. See [the code](https://github.com/KaimingHe/resnet-1k-layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future works\n",
    "\n",
    "- Representation\n",
    "    - Study of tradeoff between depth and width\n",
    "        - Potential exists for more optimal performance in shallower, wider networks (but difficult to converge)\n",
    "- Generalization\n",
    "    - Dropout\n",
    "    - MaxOut\n",
    "- Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "*Features matter*\n",
    "\n",
    "- Deeper features are more transferable to other recognition tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN pipe__\n",
    "\n",
    "![Image](img/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Acknowledgements__\n",
    "\n",
    "The concept of ResNet is not limited to computer vision; it is suitable for other recognition tasks, such as:\n",
    "\n",
    "- Speech recognition\n",
    "- Image generation\n",
    "- NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Takeaways__:\n",
    "\n",
    "- Deeper is better\n",
    "- Features matter\n",
    "- Fast R-CNN is simply amazing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
