{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "[David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html) - Google DeepMind\n",
    "\n",
    "06/19/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight sharing\n",
    "\n",
    "__Over time__\n",
    "- Recurrent Neural Networks\n",
    "  - Apply weights from *t<sub>0</sub>* to *t<sub>1</sub>*, ad nauseum\n",
    "\n",
    "__Over space__\n",
    "- Convolutional Neural Network\n",
    "  - Shares weights between local regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "*The science of optimal decision making*\n",
    "\n",
    "- Covers many fields:\n",
    "  - Optimization\n",
    "  - Game theory\n",
    "  - Dynamic programming\n",
    "  \n",
    "__Agent (brain) and Environment (world)__\n",
    "- At each step the agent:\n",
    "  - executes some action\n",
    "  - receives an observation\n",
    "  - receives a scalar reward\n",
    "- The experience is a sequence of observations, actions, rewards.\n",
    "- The state is a summary of experiences\n",
    "\n",
    "__Major components__\n",
    "1. Policy\n",
    "  - The agent's behaviour\n",
    "  - A map from state to action\n",
    "2. Value function\n",
    "  - A prediction of a future reward\n",
    "    - *\"How much reward will I get from action A in state S\"*\n",
    "    - Q-value function is total expected reward\n",
    "      - From state *a* and action *s*\n",
    "      - Under policy π\n",
    "      - With discount factor GAMMA\n",
    "    - Value functions decompose into Bellman equations\n",
    "    - An optimal function is the maximum achievable value\n",
    "      - Q\\*(s,a) = max(Q<sup>π</sup>(s,a) = Q<sup>π\\*</sup>(s,a)\n",
    "    - Once we have Q* we can act optimally:\n",
    "      - π<sup>\\*</sup>(s) = argmax Q<sup>\\*</sup>(s,a)\n",
    "3. Model\n",
    "    - The agent's proxy for the environment\n",
    "    - Learnt from experience\n",
    "    - Planner interacts with the model using lookahead search\n",
    "    \n",
    "__Three approaches__\n",
    "1. Value-based\n",
    "  - Estimate the optimal value function\n",
    "2. Policy-based\n",
    "  - Don't search for optimal value, but the optimal policy\n",
    "3. Model-based\n",
    "  - Build a model and plan to use lookahead search to optimize reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep reinforcement learning\n",
    "\n",
    "Use deep neural networks to represent\n",
    "1. Value function\n",
    "2. Policy\n",
    "3. Model\n",
    "\n",
    "Optimize loss function by SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-based Deep RL\n",
    "\n",
    "__Deep reinforcement learning in Atari__\n",
    "\n",
    "- End-to-end leaning of values *Q(s, a)* from pixels *s*\n",
    "- Input state *s* is a stack of raw pixels from last four frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep reinforcement (DQN) examples](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Improvements since Nature DQN__\n",
    "\n",
    "1. Double DQN&mdash;remove upward bias caused by *max Q(s,a,w)*\n",
    "    - Current Q-network __w__ is used to select actions\n",
    "    - Older Q-network __w__ is used to evaluate actions\n",
    "  \n",
    "2. Prioritized replay&mdash;weight experience according to surprise\n",
    "    - Store experience in priority queue according to DQN error\n",
    "  \n",
    "3. Dueling network&mdash;split Q-network into two channels\n",
    "    - Action-independent value function *V(s,v)*\n",
    "    - Action-dependent advantage function *A(s,a,w)*\n",
    "    \n",
    "__Gorilla (General Reinforcement Learning Architecture)__\n",
    "- Distributed DL reinforcement learning\n",
    "  - Exploits multithreading of standard CPU\n",
    "  - *Parallelism actually decorrelates the data(!!!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based Deep RL\n",
    "\n",
    "- Represent policy by deep network with weights __u__\n",
    "  - *a=π(a|s, __u__)* or *a=π(s,__u__)*\n",
    "  \n",
    "- Define objective function as total discounted reward\n",
    "\n",
    "\n",
    "__Actor-Critic algorithm__\n",
    "\n",
    "Estimate value functions $$Q(s,a,w) \\approx Q^{\\pi}(s,a)$$\n",
    "  - Update policy parameters u by stochastic gradient ascent\n",
    "  \n",
    "  \n",
    "__Asynchronous Advantage Actor-Critic__\n",
    "\n",
    "1. Estimate state-value function V(s,v)\n",
    "  - Q-value estimated by an *n*-step sample\n",
    "  \n",
    "2. Actor is updated towards target\n",
    "3. Critic is updated to minimize MSE w.r.t. target\n",
    "\n",
    "\n",
    "__A3C in [Labyrinth](https://deepmind.com/blog)__\n",
    "- Conv. net\n",
    "- Needs to recollect state (i.e., \"what's behind me?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous action spaces\n",
    "\n",
    "How can we deal with high-dimensional continuous action spaces?\n",
    "  - Can't easily compute $$\\underset{a}{max}Q(s,a)$$\n",
    "    - Actor-critic algorithms learn without max\n",
    "  - Q-values are differentiable w.r.t *a*\n",
    "    - Deterministic policy gradients exploit knowledge of $$\\frac{\\partial Q}{\\partial a}$$\n",
    "    \n",
    "__DPG__ is the continuous analogue of DQN\n",
    "  - Experience replay: build data-set from agent's experience\n",
    "  - Critic estimates value of current policy by DQN\n",
    "    - (to deal with non-stationarity, u, w, are held fixed)\n",
    "  - Actor updates policy in direction that improves Q\n",
    "  \n",
    "__A3C in simulated physics demo__\n",
    "- Asynchronous RL is viable alternative to experience replay\n",
    "- Train a hierarchical, recurrent locomotion controller\n",
    "- Retrain controller on more challenging tasks\n",
    "\n",
    "\n",
    "### Fictitious self-play (FSP)\n",
    "\n",
    "- Can deep RL find Nash equilibria in multi-agent games?\n",
    "  - Q-network learns \"best response\" to opponent policies\n",
    "    - By applying DQN with experience replay\n",
    "  - Policy network π(a|s,__u__) learns an average of best responses:\n",
    "  \n",
    "  $$\\frac{\\partial l}{\\partial u} = \\frac{\\partial \\log\\pi(\\alpha |s, u)}{\\partial u}$$\n",
    "  \n",
    "  - Actions *a* sample mix of policy network and best response\n",
    "  \n",
    "\n",
    "## Model-based Deep RL\n",
    "\n",
    "__Caveat:__ errors compound!! Model-based deep RL is weak in the sense that it cannot handle situations it has not seen before.\n",
    "\n",
    "But, what happens if we have a perfect model?\n",
    "\n",
    "__Go__:\n",
    "Game tree complexity = *b<sup>d</sup>* (roughly 200<sup>200</sup>)\n",
    "\n",
    "Brute force search intractable:\n",
    "  1. Search space is huge\n",
    "  2. \"Impossible\" for computes to evaluate who is winning\n",
    "  \n",
    "Solved using CNNs\n",
    "  - Input space is like an image\n",
    "  - Output space (where to play next stone) is also like an image, or a probability distribution\n",
    "  - Two different neural networks:\n",
    "    1. Represent value function\n",
    "    2. Represent policy\n",
    "    \n",
    "Combined deep RL and supervised learning\n",
    "  - To get off the ground, studied human plays\n",
    "  - To train reinforcement, had it play against itself 1000s of times, which generates a labeled dataset\n",
    "  - Finally, train a value network\n",
    "  \n",
    "__Results__:\n",
    "1 week on 50 GPUs with semi-supervised learning: 80% vs. pure supervised learning (4 wks) 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Must break correlations in the data. Playing simply one square off is very highly correlated, so had to brute force teach the machine *not* to play this way.\n",
    "\n",
    "__Search space challenge__\n",
    "Search tree truncates after 3-4 nodes and queries value tree to determine who would have won in that situation (rather than theoretically playing the game out at each stage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
