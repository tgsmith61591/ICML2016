{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Methods for Large-Scale Machine learning\n",
    "\n",
    "[Leon Bottou](http://leon.bottou.org/) - Facebook AI Research\n",
    "\n",
    "Frank E. Curtis - Lehigh University\n",
    "\n",
    "[Jorge Nocedal](http://users.iems.northwestern.edu/~nocedal/) - Northwestern University\n",
    "\n",
    "06/19/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abstract__\n",
    "\n",
    "*This tutorial provides an accessible introduction to the \n",
    "mathematical properties of stochastic gradient methods \n",
    "and their consequences for large scale machine \n",
    "learning.  After reviewing the computational needs for \n",
    "solving optimization problems in two typical examples \n",
    "of large scale machine learning, namely, the training of \n",
    "sparse linear classifiers and deep neural networks, we \n",
    "present the theory of the simple, yet versatile stochastic \n",
    "gradient algorithm, explain its theoretical and practical \n",
    "behavior, and expose the opportunities available for \n",
    "designing improved algorithms.  We then provide \n",
    "specific examples of advanced algorithms to illustrate \n",
    "the two essential directions for improving stochastic \n",
    "gradient methods, namely, managing the noise and \n",
    "making use of second order information.*\n",
    "\n",
    "__Paper:__\n",
    "\n",
    "[Optimization Methods for Large-Scale Machine Learning](http://arxiv.org/pdf/1606.04838v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient (SG) method\n",
    "\n",
    "- Why has it risen to such prominence?\n",
    "- What is the main mechanism that drives it?\n",
    "- What can we say about its behavior in convex and non-convex optimization problems?\n",
    "\n",
    "__Organization__\n",
    "\n",
    "1. Motivation for SG method\n",
    "2. Analysis of SG\n",
    "3. How can we improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "- Given a training set, *{(x<sub>i</sub>, y<sub>i</sub>)...(x<sub>n</sub>, y<sub>n</sub>)}*\n",
    "- Given a loss function, *f(h, y)*\n",
    "- Find a prediction function *h(x; w)*\n",
    "- Optimize empirical and expected risk with respect with *w*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal definitions\n",
    "\n",
    "### Expected risk\n",
    "$$R(w) = \\int_{\\mathbb{R}^{d}x\\times \\mathbb{R}^{d}y} \\ell(h(x;w),y)dP(x,w) = \\mathbb{E}\\left [ \\ell(h(x;w),y) \\right ]$$\n",
    "\n",
    "This shows explicitly how the expected risk (and, as we'll see, empirical risk) depend on the loss function, sample space or sample set, etc.  However, when discussing optimization methods, we will often employ a simplied notation that also offers some avenues for generalizing certain algorithmic ideas. In particular, where ξ denotes a single sample, i.e., a single *(x<sub>i</sub>,y<sub>i</sub>)* or a realization of ξ may be a set of samples, i.e., \n",
    "$$\\left \\{ (x_{i},y_{i}) \\right \\}_{i}\\in S$$\n",
    "\n",
    "\n",
    "### Simplified expected risk\n",
    "\n",
    "\n",
    "In this manner, the expected risk for a given *w* is the expected value of this composite function taken with respect to the distribution of ξ:\n",
    "\n",
    "$$R(w)=\\mathbb{E} \\left [ f(w;\\xi ) \\right ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical risk\n",
    "$$R_{n}(w)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(h(x_{i};w]),y_{i})$$\n",
    "\n",
    "### Simplified empirical risk\n",
    "$$R_{n}(w) = \\frac{1}{n}\\sum_{i=1}^{n}f_{i}(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SGD algorithm:\n",
    "\n",
    "![SGD pseudo](img/sgd-pseudo.png)\n",
    "\n",
    "_*SGM is not limited to a descent method_\n",
    "\n",
    "\n",
    "\n",
    "### The argument for a batch gradient method\n",
    "\n",
    "- More expensive every step\n",
    "- Can choose among a wide range of optimization algorithms\n",
    "- Opportunities for parallelism\n",
    "    - Essentially the sum of each individual gradient, so can be solved in parallel\n",
    "    - To avoid overfitting can stop early\n",
    "\n",
    "*In theory, better; in practice, cannot compete with SGM*\n",
    "\n",
    "\n",
    "![sgm vs batch](img/sgm-vs-batch.png)\n",
    "It's is worthwhile to mention that the fast initial improvement achieved by SG,\n",
    "followed by a drastic slowdown after 1 or 2 epochs, is common in practice and is fairly well understood\n",
    "\n",
    "\n",
    "### Why is SGM so good?\n",
    "\n",
    "- Employs information more efficiently than batch method\n",
    "\n",
    "    - Argument 1\n",
    "        - Suppose data is 10 copies of a set S\n",
    "        - Iteration of batch method 10 times more expensive\n",
    "        - SG performs same computations\n",
    "        \n",
    "    - Argument 2\n",
    "        - Training set (40%), test set (30%)\n",
    "            - Why not 10%? 1%?\n",
    "            - SGM rapidly minimizes error (fewer epochs), but slows down very quickly\n",
    "                - Batch method has a linear rate of convergence\n",
    "                    - Contingent on the dimensionality of the dataset\n",
    "                    - Requires *n*log(1/eps) to be within eps of the min\n",
    "                - SGM has a sublinear rate of `1/k` for convergence\n",
    "                    - Not dependent on the dimensionality of the dataset\n",
    "                    - Requires 1/eps\n",
    "                - Why?\n",
    "                    - Cheap movement down gradient\n",
    "                    - \"The area of confusion\" (TAC)&mdash;an area of high concentration of quadratic gradients\n",
    "                    - If you diminish the step length at each iteration, you reduce the noise and reach convergence\n",
    "                        - Very difficult to tune\n",
    "                    - Fixed step lengths more simple to tune in practice\n",
    "                    \n",
    "\n",
    "__Example__ Suppose  that  each *f<sub>i</sub>* is a convex quadratic with a minimal value at zero and minimizers _w<sub>i,*</sub>_ evenly  distributed  in *[-1, 1]* such that the minimizer of *R<sub>n</sub>* is *w = 0*. At *w << -1*, SG will, with certainty, move to the right (toward _w<sub>*</sub>_).  Even if a subsequent iterate lies slightly to the right of the minimizer _w<sub>1,*</sub>_ of the \"leftmost\" quadratic, it is likely (but not certain) that SG will continue moving to the right. However, as iterates near _w<sub>*</sub>_, the algorithm enters a __region of confusion__ in which there is a signi cant chance that a step will not move toward _w<sub>*</sub>_.  In this manner, progress will slow significantly.  Only with more complete gradient information could the method know with certainty how to move toward _w<sub>*</sub>_.\n",
    "\n",
    "![many gradient example](img/many-gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGM Summary\n",
    "1. Summary\n",
    "2. Fundamental lemmas\n",
    "3. SG for strongly convex objectives\n",
    "\n",
    "__SGM Algorithm__\n",
    "![Image](img/algo4-1.png)\n",
    "\n",
    "\n",
    "Stochastic processes\n",
    "\n",
    "- We assume that the ξ<sub>k</sub> are jointly independent to avoid the full machinery of stochastic processes. but everything still holds if the ξ<sub>k</sub> form an adapted stochastic process, where each ξ<sub>k</sub> can depend on the previous one.\n",
    "\n",
    "__Smoothness__\n",
    "\n",
    "- Analysis relies on a smoothness assumption. we chose this path because it also gives results for the nonconvex case.\n",
    "\n",
    "![paper-snippet-1](img/paper-snippet-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work complexity for large-scale learning\n",
    "\n",
    "Assume that we are in the large data regime\n",
    "- Training data is unlimited\n",
    "- Computational time is limited\n",
    "\n",
    "__The good__\n",
    "- More data = less overfitting\n",
    "- Less overfitting = richer models\n",
    "\n",
    "__The bad__\n",
    "- Using more data quickly exhausts time budget\n",
    "\n",
    "__The hope__\n",
    "- How throughly do we need to optimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond SG: noise reduction and second-order methods\n",
    "\n",
    "__Assumptions__\n",
    "- With a fixed stepsize, we get a linear convergence rate to the neighborhood of the minimum\n",
    "- With a dynamic stepsize, we get a sublinear convergence rate to the minimum\n",
    "\n",
    "\n",
    "__Improving convergence rate for SG__\n",
    "- Stochastic vector noise reduction\n",
    "- Second order methods (stochastic Newton)\n",
    "\n",
    "__Non-convex objectives__\n",
    "Despite loss of convergence rate, motivation for nonconvex problems as well:\n",
    "- Convex results describe behaviors near strong local minima\n",
    "\n",
    "### Noise reduction methods\n",
    "Choosing a step size to minimize upper bound\n",
    "\n",
    "1. Fast initial improvement with SG\n",
    "  - Improve cheaply as long as you are outside of region of confusion\n",
    "2. Long-term linear rate achieved by batch gradient\n",
    "\n",
    "How to get best of both?\n",
    "- Use mini-batching as convergence slows on SG\n",
    "- Increase size of batches\n",
    "\n",
    "__How to achieve this?__\n",
    "- Mini-batching that geometrically increases with *k*\n",
    "\n",
    "But is it too fast? What about work complexity?\n",
    "- Same as SG subject to some constraints...\n",
    "\n",
    "__Considerations__\n",
    "Choosing this *T* (Tau) is quite a challenge\n",
    "- What about an adaptive technique?\n",
    "- Guarantee descent in expectation\n",
    "- Methods exist, but need geometric sample size increase as backup\n",
    "\n",
    "\"I'm minimizing a finite sum and am willing to store previous gradient(s).\"\n",
    " - reuse and revise\n",
    " - [SVRG](http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf)\n",
    "     - Stochastic variance reduced gradient method\n",
    "     1. Take full gradient step\n",
    "     2. Choose index randomly, correct component of sum with new idx\n",
    "         - Just modify one component at a time\n",
    "     - Stores entire gradient\n",
    "     - ![algorithm SVRG](img/svrg.png)\n",
    "     - Linear convergence\n",
    " - [SAGA](http://www.di.ens.fr/~fbach/Defazio_NIPS2014.pdf)\n",
    "     - Stochastic Average GrAdient method\n",
    "     - Compute full gradient step\n",
    "     - ![saga algorithm](img/saga.png)\n",
    "     - Must make considerations on storage of matrices\n",
    "     \n",
    "__Iterative averaging__\n",
    "Averages of SG iterates are less noisy\n",
    "\n",
    "\n",
    "### Second order (Newton) methods\n",
    "\n",
    "__*Neither SG nor batch gradient are invariant to linear transformations*__ (scale invariance)\n",
    "\n",
    "Batch gradient step doesn't respect curvature of quadratic function. After Newton scaling, point directly towards global minimum and converge in one step.\n",
    "\n",
    "What is known about newton's method for deteministic optimization?\n",
    "- Local rescaling based on inverse Hessian information\n",
    "- Locally quadratically vonvergent near a strong minimizer\n",
    "- Global vonvergence rate better than gradient method (when regulatized)\n",
    "\n",
    "However, is way too expensive in our case\n",
    "- Not all is lost, scaling is viable\n",
    "- Wide variety of scaling techniques improve performance\n",
    "\n",
    "__1. Inexact Hessian-free Newton__\n",
    "- Compute newton-like step\n",
    "  \n",
    "__2. (Generalized) Gauss-Newton__\n",
    "- Classical approach for nonlinear least squares, linearize inside of loss/cost\n",
    "- Leads to Gauss-Newton approximation for second-order terms\n",
    "- Can be generatlized for other (convex) losses:\n",
    "- Benefit: matrices are *always* positive (semi) definite\n",
    "\n",
    "__3. (Limited memory) quasi-Newton__\n",
    "- Only approximate second-order information with gradient displacements\n",
    "- LBFGS\n",
    "  - What is known in deterministic case\n",
    "    - Local rescaling, but using displacements in the gradients and not derivatives\n",
    "  - Super-linear convergence rate\n",
    "  - Challenges\n",
    "    - Gradients you're dealing with are not exact\n",
    "    - May not retain positive definitiveness\n",
    "    - Coming up with Hessian estimates from gradient, and inducing collinearity\n",
    "    - Updates in Hessian approximations is an overwriting process&mdash;a bad update can linger for several iterations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts:\n",
    "\n",
    "I *strongly* recommend you read [the paper](http://arxiv.org/pdf/1606.04838v1.pdf). This was an extremely dense lecture, and a very difficult one to condense into a few notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
